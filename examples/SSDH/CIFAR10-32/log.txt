I1019 11:54:18.659206  8372 caffe.cpp:99] Use GPU with device ID 0
I1019 11:54:18.829535  8372 caffe.cpp:107] Starting Optimization
I1019 11:54:18.829635  8372 solver.cpp:32] Initializing solver from parameters: 
train_net: "KevinNet_CIFAR10_32_train.prototxt"
test_net: "KevinNet_CIFAR10_32_test.prototxt"
test_iter: 100
test_interval: 100
base_lr: 0.001
display: 20
max_iter: 50000
lr_policy: "step"
gamma: 0.1
momentum: 0.9
weight_decay: 0.0005
stepsize: 25000
snapshot: 2000
snapshot_prefix: "KevinNet_CIFAR10_32"
I1019 11:54:18.829665  8372 solver.cpp:58] Creating training net from train_net file: KevinNet_CIFAR10_32_train.prototxt
I1019 11:54:18.830291  8372 net.cpp:39] Initializing net from parameters: 
name: "KevinNet_CIFAR10"
layers {
  top: "data"
  top: "label"
  name: "data"
  type: DATA
  data_param {
    source: "/home/iis/deep/rcnn_packages/caffe-new/examples/mycifar10/cifar10_train_leveldb"
    batch_size: 32
  }
  transform_param {
    mirror: true
    crop_size: 227
    mean_file: "/home/iis/deep/rcnn_packages/caffe-new/data/ilsvrc12/imagenet_mean.binaryproto"
  }
}
layers {
  bottom: "data"
  top: "conv1"
  name: "conv1"
  type: CONVOLUTION
  blobs_lr: 1
  blobs_lr: 2
  weight_decay: 1
  weight_decay: 0
  convolution_param {
    num_output: 96
    kernel_size: 11
    stride: 4
    weight_filler {
      type: "gaussian"
      std: 0.01
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layers {
  bottom: "conv1"
  top: "conv1"
  name: "relu1"
  type: RELU
}
layers {
  bottom: "conv1"
  top: "pool1"
  name: "pool1"
  type: POOLING
  pooling_param {
    pool: MAX
    kernel_size: 3
    stride: 2
  }
}
layers {
  bottom: "pool1"
  top: "norm1"
  name: "norm1"
  type: LRN
  lrn_param {
    local_size: 5
    alpha: 0.0001
    beta: 0.75
  }
}
layers {
  bottom: "norm1"
  top: "conv2"
  name: "conv2"
  type: CONVOLUTION
  blobs_lr: 1
  blobs_lr: 2
  weight_decay: 1
  weight_decay: 0
  convolution_param {
    num_output: 256
    pad: 2
    kernel_size: 5
    group: 2
    weight_filler {
      type: "gaussian"
      std: 0.01
    }
    bias_filler {
      type: "constant"
      value: 1
    }
  }
}
layers {
  bottom: "conv2"
  top: "conv2"
  name: "relu2"
  type: RELU
}
layers {
  bottom: "conv2"
  top: "pool2"
  name: "pool2"
  type: POOLING
  pooling_param {
    pool: MAX
    kernel_size: 3
    stride: 2
  }
}
layers {
  bottom: "pool2"
  top: "norm2"
  name: "norm2"
  type: LRN
  lrn_param {
    local_size: 5
    alpha: 0.0001
    beta: 0.75
  }
}
layers {
  bottom: "norm2"
  top: "conv3"
  name: "conv3"
  type: CONVOLUTION
  blobs_lr: 1
  blobs_lr: 2
  weight_decay: 1
  weight_decay: 0
  convolution_param {
    num_output: 384
    pad: 1
    kernel_size: 3
    weight_filler {
      type: "gaussian"
      std: 0.01
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layers {
  bottom: "conv3"
  top: "conv3"
  name: "relu3"
  type: RELU
}
layers {
  bottom: "conv3"
  top: "conv4"
  name: "conv4"
  type: CONVOLUTION
  blobs_lr: 1
  blobs_lr: 2
  weight_decay: 1
  weight_decay: 0
  convolution_param {
    num_output: 384
    pad: 1
    kernel_size: 3
    group: 2
    weight_filler {
      type: "gaussian"
      std: 0.01
    }
    bias_filler {
      type: "constant"
      value: 1
    }
  }
}
layers {
  bottom: "conv4"
  top: "conv4"
  name: "relu4"
  type: RELU
}
layers {
  bottom: "conv4"
  top: "conv5"
  name: "conv5"
  type: CONVOLUTION
  blobs_lr: 1
  blobs_lr: 2
  weight_decay: 1
  weight_decay: 0
  convolution_param {
    num_output: 256
    pad: 1
    kernel_size: 3
    group: 2
    weight_filler {
      type: "gaussian"
      std: 0.01
    }
    bias_filler {
      type: "constant"
      value: 1
    }
  }
}
layers {
  bottom: "conv5"
  top: "conv5"
  name: "relu5"
  type: RELU
}
layers {
  bottom: "conv5"
  top: "pool5"
  name: "pool5"
  type: POOLING
  pooling_param {
    pool: MAX
    kernel_size: 3
    stride: 2
  }
}
layers {
  bottom: "pool5"
  top: "fc6"
  name: "fc6"
  type: INNER_PRODUCT
  blobs_lr: 1
  blobs_lr: 2
  weight_decay: 1
  weight_decay: 0
  inner_product_param {
    num_output: 4096
    weight_filler {
      type: "gaussian"
      std: 0.005
    }
    bias_filler {
      type: "constant"
      value: 1
    }
  }
}
layers {
  bottom: "fc6"
  top: "fc6"
  name: "relu6"
  type: RELU
}
layers {
  bottom: "fc6"
  top: "fc6"
  name: "drop6"
  type: DROPOUT
  dropout_param {
    dropout_ratio: 0.5
  }
}
layers {
  bottom: "fc6"
  top: "fc7"
  name: "fc7"
  type: INNER_PRODUCT
  blobs_lr: 1
  blobs_lr: 2
  weight_decay: 1
  weight_decay: 0
  inner_product_param {
    num_output: 4096
    weight_filler {
      type: "gaussian"
      std: 0.005
    }
    bias_filler {
      type: "constant"
      value: 1
    }
  }
}
layers {
  bottom: "fc7"
  top: "fc7"
  name: "relu7"
  type: RELU
}
layers {
  bottom: "fc7"
  top: "fc7"
  name: "drop7"
  type: DROPOUT
  dropout_param {
    dropout_ratio: 0.5
  }
}
layers {
  bottom: "fc7"
  top: "fc8_kevin"
  name: "fc8_kevin"
  type: INNER_PRODUCT
  blobs_lr: 1
  blobs_lr: 2
  weight_decay: 1
  weight_decay: 0
  inner_product_param {
    num_output: 32
    weight_filler {
      type: "gaussian"
      std: 0.005
    }
    bias_filler {
      type: "constant"
      value: 1
    }
  }
}
layers {
  bottom: "fc8_kevin"
  top: "fc8_kevin_encode"
  name: "fc8_kevin_encode"
  type: SIGMOID
}
layers {
  bottom: "fc8_kevin_encode"
  bottom: "fc8_kevin_encode"
  top: "loss_binary"
  name: "loss_1"
  type: K1_EUCLIDEAN_LOSS
  loss_weight: 1
}
layers {
  bottom: "fc8_kevin_encode"
  bottom: "fc8_kevin_encode"
  top: "loss_balance"
  name: "loss_2"
  type: K2_EUCLIDEAN_LOSS
  loss_weight: 1
}
layers {
  bottom: "fc8_kevin_encode"
  top: "fc8_pascal"
  name: "fc8_pascal"
  type: INNER_PRODUCT
  blobs_lr: 10
  blobs_lr: 20
  weight_decay: 1
  weight_decay: 0
  inner_product_param {
    num_output: 10
    weight_filler {
      type: "gaussian"
      std: 0.01
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layers {
  bottom: "fc8_pascal"
  bottom: "label"
  top: "loss_classification"
  name: "loss"
  type: SOFTMAX_LOSS
  loss_weight: 1
}
state {
  phase: TRAIN
}
I1019 11:54:18.830476  8372 net.cpp:67] Creating Layer data
I1019 11:54:18.830489  8372 net.cpp:356] data -> data
I1019 11:54:18.830512  8372 net.cpp:356] data -> label
I1019 11:54:18.830523  8372 net.cpp:96] Setting up data
I1019 11:54:18.830543  8372 data_layer.cpp:45] Opening leveldb /home/iis/deep/rcnn_packages/caffe-new/examples/mycifar10/cifar10_train_leveldb
I1019 11:54:18.901713  8372 data_layer.cpp:128] output data size: 32,3,227,227
I1019 11:54:18.901764  8372 base_data_layer.cpp:36] Loading mean file from/home/iis/deep/rcnn_packages/caffe-new/data/ilsvrc12/imagenet_mean.binaryproto
I1019 11:54:18.915763  8372 net.cpp:103] Top shape: 32 3 227 227 (4946784)
I1019 11:54:18.915786  8372 net.cpp:103] Top shape: 32 1 1 1 (32)
I1019 11:54:18.915817  8372 net.cpp:67] Creating Layer conv1
I1019 11:54:18.915822  8372 net.cpp:394] conv1 <- data
I1019 11:54:18.915834  8372 net.cpp:356] conv1 -> conv1
I1019 11:54:18.915851  8372 net.cpp:96] Setting up conv1
I1019 11:54:18.917018  8372 net.cpp:103] Top shape: 32 96 55 55 (9292800)
I1019 11:54:18.917043  8372 net.cpp:67] Creating Layer relu1
I1019 11:54:18.917054  8372 net.cpp:394] relu1 <- conv1
I1019 11:54:18.917065  8372 net.cpp:345] relu1 -> conv1 (in-place)
I1019 11:54:18.917078  8372 net.cpp:96] Setting up relu1
I1019 11:54:18.917084  8372 net.cpp:103] Top shape: 32 96 55 55 (9292800)
I1019 11:54:18.917091  8372 net.cpp:67] Creating Layer pool1
I1019 11:54:18.917095  8372 net.cpp:394] pool1 <- conv1
I1019 11:54:18.917100  8372 net.cpp:356] pool1 -> pool1
I1019 11:54:18.917106  8372 net.cpp:96] Setting up pool1
I1019 11:54:18.917119  8372 net.cpp:103] Top shape: 32 96 27 27 (2239488)
I1019 11:54:18.917127  8372 net.cpp:67] Creating Layer norm1
I1019 11:54:18.917131  8372 net.cpp:394] norm1 <- pool1
I1019 11:54:18.917137  8372 net.cpp:356] norm1 -> norm1
I1019 11:54:18.917145  8372 net.cpp:96] Setting up norm1
I1019 11:54:18.917153  8372 net.cpp:103] Top shape: 32 96 27 27 (2239488)
I1019 11:54:18.917160  8372 net.cpp:67] Creating Layer conv2
I1019 11:54:18.917163  8372 net.cpp:394] conv2 <- norm1
I1019 11:54:18.917186  8372 net.cpp:356] conv2 -> conv2
I1019 11:54:18.917193  8372 net.cpp:96] Setting up conv2
I1019 11:54:18.927420  8372 net.cpp:103] Top shape: 32 256 27 27 (5971968)
I1019 11:54:18.927459  8372 net.cpp:67] Creating Layer relu2
I1019 11:54:18.927465  8372 net.cpp:394] relu2 <- conv2
I1019 11:54:18.927472  8372 net.cpp:345] relu2 -> conv2 (in-place)
I1019 11:54:18.927479  8372 net.cpp:96] Setting up relu2
I1019 11:54:18.927484  8372 net.cpp:103] Top shape: 32 256 27 27 (5971968)
I1019 11:54:18.927490  8372 net.cpp:67] Creating Layer pool2
I1019 11:54:18.927495  8372 net.cpp:394] pool2 <- conv2
I1019 11:54:18.927500  8372 net.cpp:356] pool2 -> pool2
I1019 11:54:18.927505  8372 net.cpp:96] Setting up pool2
I1019 11:54:18.927512  8372 net.cpp:103] Top shape: 32 256 13 13 (1384448)
I1019 11:54:18.927523  8372 net.cpp:67] Creating Layer norm2
I1019 11:54:18.927527  8372 net.cpp:394] norm2 <- pool2
I1019 11:54:18.927532  8372 net.cpp:356] norm2 -> norm2
I1019 11:54:18.927538  8372 net.cpp:96] Setting up norm2
I1019 11:54:18.927543  8372 net.cpp:103] Top shape: 32 256 13 13 (1384448)
I1019 11:54:18.927549  8372 net.cpp:67] Creating Layer conv3
I1019 11:54:18.927553  8372 net.cpp:394] conv3 <- norm2
I1019 11:54:18.927559  8372 net.cpp:356] conv3 -> conv3
I1019 11:54:18.927567  8372 net.cpp:96] Setting up conv3
I1019 11:54:18.957098  8372 net.cpp:103] Top shape: 32 384 13 13 (2076672)
I1019 11:54:18.957140  8372 net.cpp:67] Creating Layer relu3
I1019 11:54:18.957150  8372 net.cpp:394] relu3 <- conv3
I1019 11:54:18.957161  8372 net.cpp:345] relu3 -> conv3 (in-place)
I1019 11:54:18.957173  8372 net.cpp:96] Setting up relu3
I1019 11:54:18.957183  8372 net.cpp:103] Top shape: 32 384 13 13 (2076672)
I1019 11:54:18.957195  8372 net.cpp:67] Creating Layer conv4
I1019 11:54:18.957201  8372 net.cpp:394] conv4 <- conv3
I1019 11:54:18.957211  8372 net.cpp:356] conv4 -> conv4
I1019 11:54:18.957219  8372 net.cpp:96] Setting up conv4
I1019 11:54:18.979315  8372 net.cpp:103] Top shape: 32 384 13 13 (2076672)
I1019 11:54:18.979347  8372 net.cpp:67] Creating Layer relu4
I1019 11:54:18.979352  8372 net.cpp:394] relu4 <- conv4
I1019 11:54:18.979359  8372 net.cpp:345] relu4 -> conv4 (in-place)
I1019 11:54:18.979367  8372 net.cpp:96] Setting up relu4
I1019 11:54:18.979372  8372 net.cpp:103] Top shape: 32 384 13 13 (2076672)
I1019 11:54:18.979378  8372 net.cpp:67] Creating Layer conv5
I1019 11:54:18.979382  8372 net.cpp:394] conv5 <- conv4
I1019 11:54:18.979387  8372 net.cpp:356] conv5 -> conv5
I1019 11:54:18.979393  8372 net.cpp:96] Setting up conv5
I1019 11:54:18.994149  8372 net.cpp:103] Top shape: 32 256 13 13 (1384448)
I1019 11:54:18.994179  8372 net.cpp:67] Creating Layer relu5
I1019 11:54:18.994184  8372 net.cpp:394] relu5 <- conv5
I1019 11:54:18.994191  8372 net.cpp:345] relu5 -> conv5 (in-place)
I1019 11:54:18.994199  8372 net.cpp:96] Setting up relu5
I1019 11:54:18.994202  8372 net.cpp:103] Top shape: 32 256 13 13 (1384448)
I1019 11:54:18.994210  8372 net.cpp:67] Creating Layer pool5
I1019 11:54:18.994212  8372 net.cpp:394] pool5 <- conv5
I1019 11:54:18.994218  8372 net.cpp:356] pool5 -> pool5
I1019 11:54:18.994225  8372 net.cpp:96] Setting up pool5
I1019 11:54:18.994233  8372 net.cpp:103] Top shape: 32 256 6 6 (294912)
I1019 11:54:18.994242  8372 net.cpp:67] Creating Layer fc6
I1019 11:54:18.994246  8372 net.cpp:394] fc6 <- pool5
I1019 11:54:18.994251  8372 net.cpp:356] fc6 -> fc6
I1019 11:54:18.994258  8372 net.cpp:96] Setting up fc6
I1019 11:54:20.248720  8372 net.cpp:103] Top shape: 32 4096 1 1 (131072)
I1019 11:54:20.248760  8372 net.cpp:67] Creating Layer relu6
I1019 11:54:20.248766  8372 net.cpp:394] relu6 <- fc6
I1019 11:54:20.248775  8372 net.cpp:345] relu6 -> fc6 (in-place)
I1019 11:54:20.248782  8372 net.cpp:96] Setting up relu6
I1019 11:54:20.248787  8372 net.cpp:103] Top shape: 32 4096 1 1 (131072)
I1019 11:54:20.248793  8372 net.cpp:67] Creating Layer drop6
I1019 11:54:20.248797  8372 net.cpp:394] drop6 <- fc6
I1019 11:54:20.248803  8372 net.cpp:345] drop6 -> fc6 (in-place)
I1019 11:54:20.248808  8372 net.cpp:96] Setting up drop6
I1019 11:54:20.248832  8372 net.cpp:103] Top shape: 32 4096 1 1 (131072)
I1019 11:54:20.248842  8372 net.cpp:67] Creating Layer fc7
I1019 11:54:20.248847  8372 net.cpp:394] fc7 <- fc6
I1019 11:54:20.248852  8372 net.cpp:356] fc7 -> fc7
I1019 11:54:20.248860  8372 net.cpp:96] Setting up fc7
I1019 11:54:20.806933  8372 net.cpp:103] Top shape: 32 4096 1 1 (131072)
I1019 11:54:20.806973  8372 net.cpp:67] Creating Layer relu7
I1019 11:54:20.806980  8372 net.cpp:394] relu7 <- fc7
I1019 11:54:20.806988  8372 net.cpp:345] relu7 -> fc7 (in-place)
I1019 11:54:20.806996  8372 net.cpp:96] Setting up relu7
I1019 11:54:20.807000  8372 net.cpp:103] Top shape: 32 4096 1 1 (131072)
I1019 11:54:20.807006  8372 net.cpp:67] Creating Layer drop7
I1019 11:54:20.807010  8372 net.cpp:394] drop7 <- fc7
I1019 11:54:20.807015  8372 net.cpp:345] drop7 -> fc7 (in-place)
I1019 11:54:20.807020  8372 net.cpp:96] Setting up drop7
I1019 11:54:20.807025  8372 net.cpp:103] Top shape: 32 4096 1 1 (131072)
I1019 11:54:20.807031  8372 net.cpp:67] Creating Layer fc8_kevin
I1019 11:54:20.807039  8372 net.cpp:394] fc8_kevin <- fc7
I1019 11:54:20.807047  8372 net.cpp:356] fc8_kevin -> fc8_kevin
I1019 11:54:20.807060  8372 net.cpp:96] Setting up fc8_kevin
I1019 11:54:20.811381  8372 net.cpp:103] Top shape: 32 32 1 1 (1024)
I1019 11:54:20.811398  8372 net.cpp:67] Creating Layer fc8_kevin_encode
I1019 11:54:20.811401  8372 net.cpp:394] fc8_kevin_encode <- fc8_kevin
I1019 11:54:20.811408  8372 net.cpp:356] fc8_kevin_encode -> fc8_kevin_encode
I1019 11:54:20.811414  8372 net.cpp:96] Setting up fc8_kevin_encode
I1019 11:54:20.811419  8372 net.cpp:103] Top shape: 32 32 1 1 (1024)
I1019 11:54:20.811425  8372 net.cpp:67] Creating Layer fc8_kevin_encode_fc8_kevin_encode_0_split
I1019 11:54:20.811429  8372 net.cpp:394] fc8_kevin_encode_fc8_kevin_encode_0_split <- fc8_kevin_encode
I1019 11:54:20.811434  8372 net.cpp:356] fc8_kevin_encode_fc8_kevin_encode_0_split -> fc8_kevin_encode_fc8_kevin_encode_0_split_0
I1019 11:54:20.811442  8372 net.cpp:356] fc8_kevin_encode_fc8_kevin_encode_0_split -> fc8_kevin_encode_fc8_kevin_encode_0_split_1
I1019 11:54:20.811453  8372 net.cpp:356] fc8_kevin_encode_fc8_kevin_encode_0_split -> fc8_kevin_encode_fc8_kevin_encode_0_split_2
I1019 11:54:20.811465  8372 net.cpp:356] fc8_kevin_encode_fc8_kevin_encode_0_split -> fc8_kevin_encode_fc8_kevin_encode_0_split_3
I1019 11:54:20.811480  8372 net.cpp:356] fc8_kevin_encode_fc8_kevin_encode_0_split -> fc8_kevin_encode_fc8_kevin_encode_0_split_4
I1019 11:54:20.811488  8372 net.cpp:96] Setting up fc8_kevin_encode_fc8_kevin_encode_0_split
I1019 11:54:20.811494  8372 net.cpp:103] Top shape: 32 32 1 1 (1024)
I1019 11:54:20.811498  8372 net.cpp:103] Top shape: 32 32 1 1 (1024)
I1019 11:54:20.811501  8372 net.cpp:103] Top shape: 32 32 1 1 (1024)
I1019 11:54:20.811506  8372 net.cpp:103] Top shape: 32 32 1 1 (1024)
I1019 11:54:20.811509  8372 net.cpp:103] Top shape: 32 32 1 1 (1024)
I1019 11:54:20.811514  8372 net.cpp:67] Creating Layer loss_1
I1019 11:54:20.811518  8372 net.cpp:394] loss_1 <- fc8_kevin_encode_fc8_kevin_encode_0_split_0
I1019 11:54:20.811527  8372 net.cpp:394] loss_1 <- fc8_kevin_encode_fc8_kevin_encode_0_split_1
I1019 11:54:20.811537  8372 net.cpp:356] loss_1 -> loss_binary
I1019 11:54:20.811547  8372 net.cpp:96] Setting up loss_1
I1019 11:54:20.811563  8372 net.cpp:103] Top shape: 1 1 1 1 (1)
I1019 11:54:20.811568  8372 net.cpp:109]     with loss weight 1
I1019 11:54:20.811594  8372 net.cpp:67] Creating Layer loss_2
I1019 11:54:20.811597  8372 net.cpp:394] loss_2 <- fc8_kevin_encode_fc8_kevin_encode_0_split_2
I1019 11:54:20.811602  8372 net.cpp:394] loss_2 <- fc8_kevin_encode_fc8_kevin_encode_0_split_3
I1019 11:54:20.811609  8372 net.cpp:356] loss_2 -> loss_balance
I1019 11:54:20.811614  8372 net.cpp:96] Setting up loss_2
I1019 11:54:20.811620  8372 net.cpp:103] Top shape: 1 1 1 1 (1)
I1019 11:54:20.811625  8372 net.cpp:109]     with loss weight 1
I1019 11:54:20.811631  8372 net.cpp:67] Creating Layer fc8_pascal
I1019 11:54:20.811635  8372 net.cpp:394] fc8_pascal <- fc8_kevin_encode_fc8_kevin_encode_0_split_4
I1019 11:54:20.811658  8372 net.cpp:356] fc8_pascal -> fc8_pascal
I1019 11:54:20.811666  8372 net.cpp:96] Setting up fc8_pascal
I1019 11:54:20.811684  8372 net.cpp:103] Top shape: 32 10 1 1 (320)
I1019 11:54:20.811696  8372 net.cpp:67] Creating Layer loss
I1019 11:54:20.811700  8372 net.cpp:394] loss <- fc8_pascal
I1019 11:54:20.811705  8372 net.cpp:394] loss <- label
I1019 11:54:20.811710  8372 net.cpp:356] loss -> loss_classification
I1019 11:54:20.811717  8372 net.cpp:96] Setting up loss
I1019 11:54:20.811727  8372 net.cpp:103] Top shape: 1 1 1 1 (1)
I1019 11:54:20.811730  8372 net.cpp:109]     with loss weight 1
I1019 11:54:20.811735  8372 net.cpp:170] loss needs backward computation.
I1019 11:54:20.811739  8372 net.cpp:170] fc8_pascal needs backward computation.
I1019 11:54:20.811743  8372 net.cpp:170] loss_2 needs backward computation.
I1019 11:54:20.811746  8372 net.cpp:170] loss_1 needs backward computation.
I1019 11:54:20.811750  8372 net.cpp:170] fc8_kevin_encode_fc8_kevin_encode_0_split needs backward computation.
I1019 11:54:20.811754  8372 net.cpp:170] fc8_kevin_encode needs backward computation.
I1019 11:54:20.811758  8372 net.cpp:170] fc8_kevin needs backward computation.
I1019 11:54:20.811761  8372 net.cpp:170] drop7 needs backward computation.
I1019 11:54:20.811765  8372 net.cpp:170] relu7 needs backward computation.
I1019 11:54:20.811769  8372 net.cpp:170] fc7 needs backward computation.
I1019 11:54:20.811772  8372 net.cpp:170] drop6 needs backward computation.
I1019 11:54:20.811775  8372 net.cpp:170] relu6 needs backward computation.
I1019 11:54:20.811779  8372 net.cpp:170] fc6 needs backward computation.
I1019 11:54:20.811782  8372 net.cpp:170] pool5 needs backward computation.
I1019 11:54:20.811786  8372 net.cpp:170] relu5 needs backward computation.
I1019 11:54:20.811790  8372 net.cpp:170] conv5 needs backward computation.
I1019 11:54:20.811794  8372 net.cpp:170] relu4 needs backward computation.
I1019 11:54:20.811797  8372 net.cpp:170] conv4 needs backward computation.
I1019 11:54:20.811801  8372 net.cpp:170] relu3 needs backward computation.
I1019 11:54:20.811805  8372 net.cpp:170] conv3 needs backward computation.
I1019 11:54:20.811808  8372 net.cpp:170] norm2 needs backward computation.
I1019 11:54:20.811812  8372 net.cpp:170] pool2 needs backward computation.
I1019 11:54:20.811815  8372 net.cpp:170] relu2 needs backward computation.
I1019 11:54:20.811820  8372 net.cpp:170] conv2 needs backward computation.
I1019 11:54:20.811823  8372 net.cpp:170] norm1 needs backward computation.
I1019 11:54:20.811826  8372 net.cpp:170] pool1 needs backward computation.
I1019 11:54:20.811830  8372 net.cpp:170] relu1 needs backward computation.
I1019 11:54:20.811833  8372 net.cpp:170] conv1 needs backward computation.
I1019 11:54:20.811837  8372 net.cpp:172] data does not need backward computation.
I1019 11:54:20.811841  8372 net.cpp:208] This network produces output loss_balance
I1019 11:54:20.811844  8372 net.cpp:208] This network produces output loss_binary
I1019 11:54:20.811848  8372 net.cpp:208] This network produces output loss_classification
I1019 11:54:20.811864  8372 net.cpp:467] Collecting Learning Rate and Weight Decay.
I1019 11:54:20.811872  8372 net.cpp:219] Network initialization done.
I1019 11:54:20.811875  8372 net.cpp:220] Memory required for data: 219554572
I1019 11:54:20.812340  8372 solver.cpp:151] Creating test net (#0) specified by test_net file: KevinNet_CIFAR10_32_test.prototxt
I1019 11:54:20.812510  8372 net.cpp:39] Initializing net from parameters: 
name: "KevinNet_CIFAR10"
layers {
  top: "data"
  top: "label"
  name: "data"
  type: DATA
  data_param {
    source: "/home/iis/deep/rcnn_packages/caffe-new/examples/mycifar10/cifar10_val_leveldb"
    batch_size: 32
  }
  transform_param {
    mirror: true
    crop_size: 227
    mean_file: "/home/iis/deep/rcnn_packages/caffe-new/data/ilsvrc12/imagenet_mean.binaryproto"
  }
}
layers {
  bottom: "data"
  top: "conv1"
  name: "conv1"
  type: CONVOLUTION
  blobs_lr: 1
  blobs_lr: 2
  weight_decay: 1
  weight_decay: 0
  convolution_param {
    num_output: 96
    kernel_size: 11
    stride: 4
    weight_filler {
      type: "gaussian"
      std: 0.01
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layers {
  bottom: "conv1"
  top: "conv1"
  name: "relu1"
  type: RELU
}
layers {
  bottom: "conv1"
  top: "pool1"
  name: "pool1"
  type: POOLING
  pooling_param {
    pool: MAX
    kernel_size: 3
    stride: 2
  }
}
layers {
  bottom: "pool1"
  top: "norm1"
  name: "norm1"
  type: LRN
  lrn_param {
    local_size: 5
    alpha: 0.0001
    beta: 0.75
  }
}
layers {
  bottom: "norm1"
  top: "conv2"
  name: "conv2"
  type: CONVOLUTION
  blobs_lr: 1
  blobs_lr: 2
  weight_decay: 1
  weight_decay: 0
  convolution_param {
    num_output: 256
    pad: 2
    kernel_size: 5
    group: 2
    weight_filler {
      type: "gaussian"
      std: 0.01
    }
    bias_filler {
      type: "constant"
      value: 1
    }
  }
}
layers {
  bottom: "conv2"
  top: "conv2"
  name: "relu2"
  type: RELU
}
layers {
  bottom: "conv2"
  top: "pool2"
  name: "pool2"
  type: POOLING
  pooling_param {
    pool: MAX
    kernel_size: 3
    stride: 2
  }
}
layers {
  bottom: "pool2"
  top: "norm2"
  name: "norm2"
  type: LRN
  lrn_param {
    local_size: 5
    alpha: 0.0001
    beta: 0.75
  }
}
layers {
  bottom: "norm2"
  top: "conv3"
  name: "conv3"
  type: CONVOLUTION
  blobs_lr: 1
  blobs_lr: 2
  weight_decay: 1
  weight_decay: 0
  convolution_param {
    num_output: 384
    pad: 1
    kernel_size: 3
    weight_filler {
      type: "gaussian"
      std: 0.01
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layers {
  bottom: "conv3"
  top: "conv3"
  name: "relu3"
  type: RELU
}
layers {
  bottom: "conv3"
  top: "conv4"
  name: "conv4"
  type: CONVOLUTION
  blobs_lr: 1
  blobs_lr: 2
  weight_decay: 1
  weight_decay: 0
  convolution_param {
    num_output: 384
    pad: 1
    kernel_size: 3
    group: 2
    weight_filler {
      type: "gaussian"
      std: 0.01
    }
    bias_filler {
      type: "constant"
      value: 1
    }
  }
}
layers {
  bottom: "conv4"
  top: "conv4"
  name: "relu4"
  type: RELU
}
layers {
  bottom: "conv4"
  top: "conv5"
  name: "conv5"
  type: CONVOLUTION
  blobs_lr: 1
  blobs_lr: 2
  weight_decay: 1
  weight_decay: 0
  convolution_param {
    num_output: 256
    pad: 1
    kernel_size: 3
    group: 2
    weight_filler {
      type: "gaussian"
      std: 0.01
    }
    bias_filler {
      type: "constant"
      value: 1
    }
  }
}
layers {
  bottom: "conv5"
  top: "conv5"
  name: "relu5"
  type: RELU
}
layers {
  bottom: "conv5"
  top: "pool5"
  name: "pool5"
  type: POOLING
  pooling_param {
    pool: MAX
    kernel_size: 3
    stride: 2
  }
}
layers {
  bottom: "pool5"
  top: "fc6"
  name: "fc6"
  type: INNER_PRODUCT
  blobs_lr: 1
  blobs_lr: 2
  weight_decay: 1
  weight_decay: 0
  inner_product_param {
    num_output: 4096
    weight_filler {
      type: "gaussian"
      std: 0.005
    }
    bias_filler {
      type: "constant"
      value: 1
    }
  }
}
layers {
  bottom: "fc6"
  top: "fc6"
  name: "relu6"
  type: RELU
}
layers {
  bottom: "fc6"
  top: "fc6"
  name: "drop6"
  type: DROPOUT
  dropout_param {
    dropout_ratio: 0.5
  }
}
layers {
  bottom: "fc6"
  top: "fc7"
  name: "fc7"
  type: INNER_PRODUCT
  blobs_lr: 1
  blobs_lr: 2
  weight_decay: 1
  weight_decay: 0
  inner_product_param {
    num_output: 4096
    weight_filler {
      type: "gaussian"
      std: 0.005
    }
    bias_filler {
      type: "constant"
      value: 1
    }
  }
}
layers {
  bottom: "fc7"
  top: "fc7"
  name: "relu7"
  type: RELU
}
layers {
  bottom: "fc7"
  top: "fc7"
  name: "drop7"
  type: DROPOUT
  dropout_param {
    dropout_ratio: 0.5
  }
}
layers {
  bottom: "fc7"
  top: "fc8_kevin"
  name: "fc8_kevin"
  type: INNER_PRODUCT
  blobs_lr: 1
  blobs_lr: 2
  weight_decay: 1
  weight_decay: 0
  inner_product_param {
    num_output: 32
    weight_filler {
      type: "gaussian"
      std: 0.005
    }
    bias_filler {
      type: "constant"
      value: 1
    }
  }
}
layers {
  bottom: "fc8_kevin"
  top: "fc8_kevin_encode"
  name: "fc8_kevin_encode"
  type: SIGMOID
}
layers {
  bottom: "fc8_kevin_encode"
  top: "fc8_pascal"
  name: "fc8_pascal"
  type: INNER_PRODUCT
  blobs_lr: 10
  blobs_lr: 20
  weight_decay: 1
  weight_decay: 0
  inner_product_param {
    num_output: 10
    weight_filler {
      type: "gaussian"
      std: 0.01
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layers {
  bottom: "fc8_pascal"
  top: "prob"
  name: "prob"
  type: SOFTMAX
}
layers {
  bottom: "prob"
  bottom: "label"
  top: "accuracy"
  name: "accuracy"
  type: ACCURACY
}
state {
  phase: TEST
}
I1019 11:54:20.812589  8372 net.cpp:67] Creating Layer data
I1019 11:54:20.812597  8372 net.cpp:356] data -> data
I1019 11:54:20.812607  8372 net.cpp:356] data -> label
I1019 11:54:20.812613  8372 net.cpp:96] Setting up data
I1019 11:54:20.812618  8372 data_layer.cpp:45] Opening leveldb /home/iis/deep/rcnn_packages/caffe-new/examples/mycifar10/cifar10_val_leveldb
I1019 11:54:20.864665  8372 data_layer.cpp:128] output data size: 32,3,227,227
I1019 11:54:20.864688  8372 base_data_layer.cpp:36] Loading mean file from/home/iis/deep/rcnn_packages/caffe-new/data/ilsvrc12/imagenet_mean.binaryproto
I1019 11:54:20.873816  8372 net.cpp:103] Top shape: 32 3 227 227 (4946784)
I1019 11:54:20.873842  8372 net.cpp:103] Top shape: 32 1 1 1 (32)
I1019 11:54:20.873857  8372 net.cpp:67] Creating Layer conv1
I1019 11:54:20.873862  8372 net.cpp:394] conv1 <- data
I1019 11:54:20.873872  8372 net.cpp:356] conv1 -> conv1
I1019 11:54:20.873884  8372 net.cpp:96] Setting up conv1
I1019 11:54:20.875015  8372 net.cpp:103] Top shape: 32 96 55 55 (9292800)
I1019 11:54:20.875031  8372 net.cpp:67] Creating Layer relu1
I1019 11:54:20.875036  8372 net.cpp:394] relu1 <- conv1
I1019 11:54:20.875041  8372 net.cpp:345] relu1 -> conv1 (in-place)
I1019 11:54:20.875047  8372 net.cpp:96] Setting up relu1
I1019 11:54:20.875054  8372 net.cpp:103] Top shape: 32 96 55 55 (9292800)
I1019 11:54:20.875064  8372 net.cpp:67] Creating Layer pool1
I1019 11:54:20.875072  8372 net.cpp:394] pool1 <- conv1
I1019 11:54:20.875082  8372 net.cpp:356] pool1 -> pool1
I1019 11:54:20.875097  8372 net.cpp:96] Setting up pool1
I1019 11:54:20.875108  8372 net.cpp:103] Top shape: 32 96 27 27 (2239488)
I1019 11:54:20.875114  8372 net.cpp:67] Creating Layer norm1
I1019 11:54:20.875118  8372 net.cpp:394] norm1 <- pool1
I1019 11:54:20.875124  8372 net.cpp:356] norm1 -> norm1
I1019 11:54:20.875130  8372 net.cpp:96] Setting up norm1
I1019 11:54:20.875138  8372 net.cpp:103] Top shape: 32 96 27 27 (2239488)
I1019 11:54:20.875149  8372 net.cpp:67] Creating Layer conv2
I1019 11:54:20.875160  8372 net.cpp:394] conv2 <- norm1
I1019 11:54:20.875170  8372 net.cpp:356] conv2 -> conv2
I1019 11:54:20.875180  8372 net.cpp:96] Setting up conv2
I1019 11:54:20.885360  8372 net.cpp:103] Top shape: 32 256 27 27 (5971968)
I1019 11:54:20.885403  8372 net.cpp:67] Creating Layer relu2
I1019 11:54:20.885412  8372 net.cpp:394] relu2 <- conv2
I1019 11:54:20.885424  8372 net.cpp:345] relu2 -> conv2 (in-place)
I1019 11:54:20.885439  8372 net.cpp:96] Setting up relu2
I1019 11:54:20.885447  8372 net.cpp:103] Top shape: 32 256 27 27 (5971968)
I1019 11:54:20.885455  8372 net.cpp:67] Creating Layer pool2
I1019 11:54:20.885462  8372 net.cpp:394] pool2 <- conv2
I1019 11:54:20.885470  8372 net.cpp:356] pool2 -> pool2
I1019 11:54:20.885480  8372 net.cpp:96] Setting up pool2
I1019 11:54:20.885489  8372 net.cpp:103] Top shape: 32 256 13 13 (1384448)
I1019 11:54:20.885503  8372 net.cpp:67] Creating Layer norm2
I1019 11:54:20.885509  8372 net.cpp:394] norm2 <- pool2
I1019 11:54:20.885519  8372 net.cpp:356] norm2 -> norm2
I1019 11:54:20.885529  8372 net.cpp:96] Setting up norm2
I1019 11:54:20.885535  8372 net.cpp:103] Top shape: 32 256 13 13 (1384448)
I1019 11:54:20.885542  8372 net.cpp:67] Creating Layer conv3
I1019 11:54:20.885546  8372 net.cpp:394] conv3 <- norm2
I1019 11:54:20.885552  8372 net.cpp:356] conv3 -> conv3
I1019 11:54:20.885558  8372 net.cpp:96] Setting up conv3
I1019 11:54:20.915230  8372 net.cpp:103] Top shape: 32 384 13 13 (2076672)
I1019 11:54:20.915303  8372 net.cpp:67] Creating Layer relu3
I1019 11:54:20.915312  8372 net.cpp:394] relu3 <- conv3
I1019 11:54:20.915319  8372 net.cpp:345] relu3 -> conv3 (in-place)
I1019 11:54:20.915326  8372 net.cpp:96] Setting up relu3
I1019 11:54:20.915331  8372 net.cpp:103] Top shape: 32 384 13 13 (2076672)
I1019 11:54:20.915338  8372 net.cpp:67] Creating Layer conv4
I1019 11:54:20.915343  8372 net.cpp:394] conv4 <- conv3
I1019 11:54:20.915349  8372 net.cpp:356] conv4 -> conv4
I1019 11:54:20.915359  8372 net.cpp:96] Setting up conv4
I1019 11:54:20.937502  8372 net.cpp:103] Top shape: 32 384 13 13 (2076672)
I1019 11:54:20.937537  8372 net.cpp:67] Creating Layer relu4
I1019 11:54:20.937543  8372 net.cpp:394] relu4 <- conv4
I1019 11:54:20.937551  8372 net.cpp:345] relu4 -> conv4 (in-place)
I1019 11:54:20.937557  8372 net.cpp:96] Setting up relu4
I1019 11:54:20.937562  8372 net.cpp:103] Top shape: 32 384 13 13 (2076672)
I1019 11:54:20.937569  8372 net.cpp:67] Creating Layer conv5
I1019 11:54:20.937573  8372 net.cpp:394] conv5 <- conv4
I1019 11:54:20.937578  8372 net.cpp:356] conv5 -> conv5
I1019 11:54:20.937588  8372 net.cpp:96] Setting up conv5
I1019 11:54:20.952358  8372 net.cpp:103] Top shape: 32 256 13 13 (1384448)
I1019 11:54:20.952379  8372 net.cpp:67] Creating Layer relu5
I1019 11:54:20.952384  8372 net.cpp:394] relu5 <- conv5
I1019 11:54:20.952389  8372 net.cpp:345] relu5 -> conv5 (in-place)
I1019 11:54:20.952395  8372 net.cpp:96] Setting up relu5
I1019 11:54:20.952399  8372 net.cpp:103] Top shape: 32 256 13 13 (1384448)
I1019 11:54:20.952405  8372 net.cpp:67] Creating Layer pool5
I1019 11:54:20.952409  8372 net.cpp:394] pool5 <- conv5
I1019 11:54:20.952419  8372 net.cpp:356] pool5 -> pool5
I1019 11:54:20.952433  8372 net.cpp:96] Setting up pool5
I1019 11:54:20.952443  8372 net.cpp:103] Top shape: 32 256 6 6 (294912)
I1019 11:54:20.952455  8372 net.cpp:67] Creating Layer fc6
I1019 11:54:20.952460  8372 net.cpp:394] fc6 <- pool5
I1019 11:54:20.952466  8372 net.cpp:356] fc6 -> fc6
I1019 11:54:20.952472  8372 net.cpp:96] Setting up fc6
I1019 11:54:22.202249  8372 net.cpp:103] Top shape: 32 4096 1 1 (131072)
I1019 11:54:22.202289  8372 net.cpp:67] Creating Layer relu6
I1019 11:54:22.202296  8372 net.cpp:394] relu6 <- fc6
I1019 11:54:22.202303  8372 net.cpp:345] relu6 -> fc6 (in-place)
I1019 11:54:22.202312  8372 net.cpp:96] Setting up relu6
I1019 11:54:22.202316  8372 net.cpp:103] Top shape: 32 4096 1 1 (131072)
I1019 11:54:22.202322  8372 net.cpp:67] Creating Layer drop6
I1019 11:54:22.202327  8372 net.cpp:394] drop6 <- fc6
I1019 11:54:22.202332  8372 net.cpp:345] drop6 -> fc6 (in-place)
I1019 11:54:22.202337  8372 net.cpp:96] Setting up drop6
I1019 11:54:22.202343  8372 net.cpp:103] Top shape: 32 4096 1 1 (131072)
I1019 11:54:22.202353  8372 net.cpp:67] Creating Layer fc7
I1019 11:54:22.202361  8372 net.cpp:394] fc7 <- fc6
I1019 11:54:22.202371  8372 net.cpp:356] fc7 -> fc7
I1019 11:54:22.202383  8372 net.cpp:96] Setting up fc7
I1019 11:54:22.757100  8372 net.cpp:103] Top shape: 32 4096 1 1 (131072)
I1019 11:54:22.757139  8372 net.cpp:67] Creating Layer relu7
I1019 11:54:22.757145  8372 net.cpp:394] relu7 <- fc7
I1019 11:54:22.757153  8372 net.cpp:345] relu7 -> fc7 (in-place)
I1019 11:54:22.757161  8372 net.cpp:96] Setting up relu7
I1019 11:54:22.757166  8372 net.cpp:103] Top shape: 32 4096 1 1 (131072)
I1019 11:54:22.757172  8372 net.cpp:67] Creating Layer drop7
I1019 11:54:22.757176  8372 net.cpp:394] drop7 <- fc7
I1019 11:54:22.757181  8372 net.cpp:345] drop7 -> fc7 (in-place)
I1019 11:54:22.757187  8372 net.cpp:96] Setting up drop7
I1019 11:54:22.757194  8372 net.cpp:103] Top shape: 32 4096 1 1 (131072)
I1019 11:54:22.757206  8372 net.cpp:67] Creating Layer fc8_kevin
I1019 11:54:22.757213  8372 net.cpp:394] fc8_kevin <- fc7
I1019 11:54:22.757223  8372 net.cpp:356] fc8_kevin -> fc8_kevin
I1019 11:54:22.757235  8372 net.cpp:96] Setting up fc8_kevin
I1019 11:54:22.761440  8372 net.cpp:103] Top shape: 32 32 1 1 (1024)
I1019 11:54:22.761454  8372 net.cpp:67] Creating Layer fc8_kevin_encode
I1019 11:54:22.761477  8372 net.cpp:394] fc8_kevin_encode <- fc8_kevin
I1019 11:54:22.761484  8372 net.cpp:356] fc8_kevin_encode -> fc8_kevin_encode
I1019 11:54:22.761492  8372 net.cpp:96] Setting up fc8_kevin_encode
I1019 11:54:22.761495  8372 net.cpp:103] Top shape: 32 32 1 1 (1024)
I1019 11:54:22.761502  8372 net.cpp:67] Creating Layer fc8_pascal
I1019 11:54:22.761507  8372 net.cpp:394] fc8_pascal <- fc8_kevin_encode
I1019 11:54:22.761513  8372 net.cpp:356] fc8_pascal -> fc8_pascal
I1019 11:54:22.761521  8372 net.cpp:96] Setting up fc8_pascal
I1019 11:54:22.761541  8372 net.cpp:103] Top shape: 32 10 1 1 (320)
I1019 11:54:22.761551  8372 net.cpp:67] Creating Layer prob
I1019 11:54:22.761557  8372 net.cpp:394] prob <- fc8_pascal
I1019 11:54:22.761562  8372 net.cpp:356] prob -> prob
I1019 11:54:22.761569  8372 net.cpp:96] Setting up prob
I1019 11:54:22.761574  8372 net.cpp:103] Top shape: 32 10 1 1 (320)
I1019 11:54:22.761579  8372 net.cpp:67] Creating Layer accuracy
I1019 11:54:22.761584  8372 net.cpp:394] accuracy <- prob
I1019 11:54:22.761589  8372 net.cpp:394] accuracy <- label
I1019 11:54:22.761593  8372 net.cpp:356] accuracy -> accuracy
I1019 11:54:22.761600  8372 net.cpp:96] Setting up accuracy
I1019 11:54:22.761610  8372 net.cpp:103] Top shape: 1 1 1 1 (1)
I1019 11:54:22.761615  8372 net.cpp:172] accuracy does not need backward computation.
I1019 11:54:22.761620  8372 net.cpp:172] prob does not need backward computation.
I1019 11:54:22.761622  8372 net.cpp:172] fc8_pascal does not need backward computation.
I1019 11:54:22.761626  8372 net.cpp:172] fc8_kevin_encode does not need backward computation.
I1019 11:54:22.761629  8372 net.cpp:172] fc8_kevin does not need backward computation.
I1019 11:54:22.761632  8372 net.cpp:172] drop7 does not need backward computation.
I1019 11:54:22.761636  8372 net.cpp:172] relu7 does not need backward computation.
I1019 11:54:22.761638  8372 net.cpp:172] fc7 does not need backward computation.
I1019 11:54:22.761641  8372 net.cpp:172] drop6 does not need backward computation.
I1019 11:54:22.761644  8372 net.cpp:172] relu6 does not need backward computation.
I1019 11:54:22.761648  8372 net.cpp:172] fc6 does not need backward computation.
I1019 11:54:22.761651  8372 net.cpp:172] pool5 does not need backward computation.
I1019 11:54:22.761654  8372 net.cpp:172] relu5 does not need backward computation.
I1019 11:54:22.761657  8372 net.cpp:172] conv5 does not need backward computation.
I1019 11:54:22.761661  8372 net.cpp:172] relu4 does not need backward computation.
I1019 11:54:22.761664  8372 net.cpp:172] conv4 does not need backward computation.
I1019 11:54:22.761667  8372 net.cpp:172] relu3 does not need backward computation.
I1019 11:54:22.761670  8372 net.cpp:172] conv3 does not need backward computation.
I1019 11:54:22.761674  8372 net.cpp:172] norm2 does not need backward computation.
I1019 11:54:22.761677  8372 net.cpp:172] pool2 does not need backward computation.
I1019 11:54:22.761680  8372 net.cpp:172] relu2 does not need backward computation.
I1019 11:54:22.761683  8372 net.cpp:172] conv2 does not need backward computation.
I1019 11:54:22.761687  8372 net.cpp:172] norm1 does not need backward computation.
I1019 11:54:22.761690  8372 net.cpp:172] pool1 does not need backward computation.
I1019 11:54:22.761693  8372 net.cpp:172] relu1 does not need backward computation.
I1019 11:54:22.761696  8372 net.cpp:172] conv1 does not need backward computation.
I1019 11:54:22.761699  8372 net.cpp:172] data does not need backward computation.
I1019 11:54:22.761703  8372 net.cpp:208] This network produces output accuracy
I1019 11:54:22.761715  8372 net.cpp:467] Collecting Learning Rate and Weight Decay.
I1019 11:54:22.761723  8372 net.cpp:219] Network initialization done.
I1019 11:54:22.761729  8372 net.cpp:220] Memory required for data: 219535364
I1019 11:54:22.761775  8372 solver.cpp:41] Solver scaffolding done.
I1019 11:54:22.761782  8372 caffe.cpp:115] Finetuning from /home/iis/deep/rcnn_packages/caffe-new/models/bvlc_reference_caffenet/bvlc_reference_caffenet.caffemodel
E1019 11:54:23.234413  8372 upgrade_proto.cpp:617] Attempting to upgrade input file specified using deprecated transformation parameters: /home/iis/deep/rcnn_packages/caffe-new/models/bvlc_reference_caffenet/bvlc_reference_caffenet.caffemodel
I1019 11:54:23.234611  8372 upgrade_proto.cpp:620] Successfully upgraded file specified using deprecated data transformation parameters.
E1019 11:54:23.234616  8372 upgrade_proto.cpp:622] Note that future Caffe releases will only support transform_param messages for transformation fields.
I1019 11:54:23.291903  8372 solver.cpp:160] Solving KevinNet_CIFAR10
I1019 11:54:23.291960  8372 solver.cpp:247] Iteration 0, Testing net (#0)
I1019 11:54:29.342694  8372 solver.cpp:298]     Test net output #0: accuracy = 0.0990625
I1019 11:54:29.500354  8372 solver.cpp:191] Iteration 0, loss = 2.29558
I1019 11:54:29.500393  8372 solver.cpp:206]     Train net output #0: loss_balance = 0.000659205 (* 1 = 0.000659205 loss)
I1019 11:54:29.500403  8372 solver.cpp:206]     Train net output #1: loss_binary = -0.000832432 (* 1 = -0.000832432 loss)
I1019 11:54:29.500411  8372 solver.cpp:206]     Train net output #2: loss_classification = 2.29575 (* 1 = 2.29575 loss)
I1019 11:54:29.500430  8372 solver.cpp:403] Iteration 0, lr = 0.001
I1019 11:54:32.998642  8372 solver.cpp:191] Iteration 20, loss = 2.3102
I1019 11:54:32.998685  8372 solver.cpp:206]     Train net output #0: loss_balance = 0.000619616 (* 1 = 0.000619616 loss)
I1019 11:54:32.998695  8372 solver.cpp:206]     Train net output #1: loss_binary = -0.000781543 (* 1 = -0.000781543 loss)
I1019 11:54:32.998703  8372 solver.cpp:206]     Train net output #2: loss_classification = 2.31036 (* 1 = 2.31036 loss)
I1019 11:54:32.998710  8372 solver.cpp:403] Iteration 20, lr = 0.001
I1019 11:54:36.496613  8372 solver.cpp:191] Iteration 40, loss = 2.3052
I1019 11:54:36.496654  8372 solver.cpp:206]     Train net output #0: loss_balance = 0.000520327 (* 1 = 0.000520327 loss)
I1019 11:54:36.496665  8372 solver.cpp:206]     Train net output #1: loss_binary = -0.000699088 (* 1 = -0.000699088 loss)
I1019 11:54:36.496672  8372 solver.cpp:206]     Train net output #2: loss_classification = 2.30538 (* 1 = 2.30538 loss)
I1019 11:54:36.496680  8372 solver.cpp:403] Iteration 40, lr = 0.001
I1019 11:54:39.994920  8372 solver.cpp:191] Iteration 60, loss = 2.29407
I1019 11:54:39.994959  8372 solver.cpp:206]     Train net output #0: loss_balance = 0.000274369 (* 1 = 0.000274369 loss)
I1019 11:54:39.994968  8372 solver.cpp:206]     Train net output #1: loss_binary = -0.000537177 (* 1 = -0.000537177 loss)
I1019 11:54:39.994976  8372 solver.cpp:206]     Train net output #2: loss_classification = 2.29433 (* 1 = 2.29433 loss)
I1019 11:54:39.994982  8372 solver.cpp:403] Iteration 60, lr = 0.001
I1019 11:54:43.497357  8372 solver.cpp:191] Iteration 80, loss = 2.28923
I1019 11:54:43.497395  8372 solver.cpp:206]     Train net output #0: loss_balance = 0.000123504 (* 1 = 0.000123504 loss)
I1019 11:54:43.497405  8372 solver.cpp:206]     Train net output #1: loss_binary = -0.000430156 (* 1 = -0.000430156 loss)
I1019 11:54:43.497411  8372 solver.cpp:206]     Train net output #2: loss_classification = 2.28954 (* 1 = 2.28954 loss)
I1019 11:54:43.497418  8372 solver.cpp:403] Iteration 80, lr = 0.001
I1019 11:54:46.820235  8372 solver.cpp:247] Iteration 100, Testing net (#0)
I1019 11:54:52.817878  8372 solver.cpp:298]     Test net output #0: accuracy = 0.238125
I1019 11:54:52.958745  8372 solver.cpp:191] Iteration 100, loss = 2.29073
I1019 11:54:52.958782  8372 solver.cpp:206]     Train net output #0: loss_balance = 1.45089e-06 (* 1 = 1.45089e-06 loss)
I1019 11:54:52.958792  8372 solver.cpp:206]     Train net output #1: loss_binary = -0.000501971 (* 1 = -0.000501971 loss)
I1019 11:54:52.958799  8372 solver.cpp:206]     Train net output #2: loss_classification = 2.29123 (* 1 = 2.29123 loss)
I1019 11:54:52.958806  8372 solver.cpp:403] Iteration 100, lr = 0.001
I1019 11:54:56.457798  8372 solver.cpp:191] Iteration 120, loss = 2.23681
I1019 11:54:56.457837  8372 solver.cpp:206]     Train net output #0: loss_balance = 8.36754e-06 (* 1 = 8.36754e-06 loss)
I1019 11:54:56.457846  8372 solver.cpp:206]     Train net output #1: loss_binary = -0.0005589 (* 1 = -0.0005589 loss)
I1019 11:54:56.457854  8372 solver.cpp:206]     Train net output #2: loss_classification = 2.23736 (* 1 = 2.23736 loss)
I1019 11:54:56.457860  8372 solver.cpp:403] Iteration 120, lr = 0.001
I1019 11:54:59.961519  8372 solver.cpp:191] Iteration 140, loss = 2.04675
I1019 11:54:59.961562  8372 solver.cpp:206]     Train net output #0: loss_balance = 4.55996e-06 (* 1 = 4.55996e-06 loss)
I1019 11:54:59.961573  8372 solver.cpp:206]     Train net output #1: loss_binary = -0.00126326 (* 1 = -0.00126326 loss)
I1019 11:54:59.961580  8372 solver.cpp:206]     Train net output #2: loss_classification = 2.048 (* 1 = 2.048 loss)
I1019 11:54:59.961588  8372 solver.cpp:403] Iteration 140, lr = 0.001
